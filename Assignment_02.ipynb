{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1=[[ 0.10872347 -0.33083632 -1.05711875 -0.51129862 -1.67806529 -1.30239494\n",
      "  -0.21643794 -1.32435208  1.68231869 -1.73737494]\n",
      " [ 0.1476422  -0.87174227  0.82310352  0.43419804  0.16996779 -0.35764023\n",
      "  -0.40539864 -1.82211265 -0.79151119 -1.3247698 ]\n",
      " [-1.23859398  0.2512468   0.45277861  0.17694935  0.73751581 -0.73533181\n",
      "  -1.11671467  0.33467223  1.97919221 -0.8873195 ]\n",
      " [ 0.60816005 -1.60646661 -0.06548942  0.41576508 -0.88194232  0.11069595\n",
      "   0.9354566   0.29287327 -1.19881468 -0.5179497 ]\n",
      " [-1.47221822 -1.28771805 -1.29754697 -1.39153744  0.96456564  0.02053379\n",
      "  -1.43246235 -0.36216013  1.96243084  0.6534755 ]\n",
      " [ 0.31932438  1.89070305 -0.7049679   0.48632318 -0.64444621 -1.06887079\n",
      "   0.81478976  0.07667409 -0.04145926  0.0852604 ]\n",
      " [ 0.79554363 -1.75475787  0.7288445  -0.66845425  0.33911663  1.38396915\n",
      "   2.14510952  1.05551914 -1.17980766 -1.52329929]\n",
      " [-0.40767594  0.42936741  2.13026683 -1.77343719  1.16056644 -1.92106326\n",
      "  -0.02087693 -1.53271873 -1.72275832 -1.38725815]\n",
      " [-0.14524857  2.68641843 -0.26083162  1.05074877  0.4588039  -0.0256431\n",
      "   1.04684807  0.13108632 -1.6849065   2.70026939]\n",
      " [-0.26334112  0.50439912  0.78221339 -0.02419517 -0.7639224  -0.39978803\n",
      "  -0.24695631  0.08581741  0.16232626 -2.54808846]\n",
      " [ 0.75098375 -1.62081694 -0.22582488  0.7636539  -1.00073454 -0.18826869\n",
      "  -1.79007219  0.38204746  1.51341304 -0.95864713]\n",
      " [ 0.20448228  1.20829265 -0.55631854  1.95168001  1.19000493  1.1870248\n",
      "  -0.02347286 -0.36672533 -0.79515804  2.41902155]\n",
      " [ 1.02733846 -0.10413657 -0.79238886 -0.80374761 -1.48010916 -2.53285684\n",
      "   0.86516519 -0.04745772 -1.12048146 -1.17193495]] \n",
      " w2=[[1.74402544]\n",
      " [1.41459535]\n",
      " [1.67823936]\n",
      " [2.99148581]\n",
      " [1.67304765]\n",
      " [1.06805279]\n",
      " [1.11551031]\n",
      " [2.45262306]\n",
      " [0.64291818]\n",
      " [2.01499802]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    使用numpy实现Boston房价预测\n",
    "    Step1 数据加载，来源sklearn中的load_boston\n",
    "    Step2 数据规范化，将X 采用正态分布规范化\n",
    "    Step3 初始化网络\n",
    "    Step4 定义激活函数，损失函数，学习率 epoch\n",
    "    Step5 循环执行：前向传播，计算损失函数，反向传播，参数更新\n",
    "    Step6 输出训练好的model参数，即w1, w2, b1, b2\n",
    "\"\"\" \n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "\n",
    "# 数据加载\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y = data['target']\n",
    "\n",
    "# 将y转化为矩阵的形式\n",
    "y = y.reshape(y.shape[0],1)\n",
    "\n",
    "# 数据规范化\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "\"\"\"\n",
    "    初始化网络参数\n",
    "    定义隐藏层维度，w1,b1,w2,b2\n",
    "\"\"\" \n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "w1 = np.random.randn(n_features, n_hidden)\n",
    "b1 = np.zeros(n_hidden)\n",
    "w2 = np.random.randn(n_hidden, 1)\n",
    "b2 = np.zeros(1)\n",
    "\n",
    "# relu函数\n",
    "def Relu(x):\n",
    "    y = np.where(x<0,0,x)\n",
    "    return y\n",
    "\n",
    "\n",
    "# 设置学习率\n",
    "learning_rate = 1e-6\n",
    "\n",
    "# 定义损失函数\n",
    "def MSE_loss(y, y_hat):\n",
    "     return np.square(y_hat-y).sum\n",
    "\n",
    "# 定义线性回归函数\n",
    "def Linear(X, W1, b1):\n",
    "    \"\"\" 这里写你的代码 \"\"\"\n",
    "    return X.dot(W1)+b1\n",
    "\n",
    "# 5000次迭代\n",
    "for t in range(5000):\n",
    "    # 前向传播，计算预测值y (Linear->Relu->Linear)\n",
    "    \"\"\" 这里写你的代码 \"\"\"\n",
    "    temp=Linear(X_,w1,b1)\n",
    "    temp_Relu=Relu(temp)\n",
    "    y_pred=Linear(temp_Relu,w2,b2)\n",
    "    \n",
    "    # 计算损失函数, 并输出每次epoch的loss\n",
    "    \"\"\" 这里写你的代码 \"\"\"\n",
    "    loss=MSE_loss(y_pred,y)\n",
    "    \n",
    "    \n",
    "    # 反向传播，基于loss 计算w1和w2的梯度\n",
    "    \"\"\" 这里写你的代码 \"\"\"\n",
    "    grad_y_pred=2.0*(y_pred-y)\n",
    "    grad_w2=temp_Relu.T.dot(grad_y_pred)\n",
    "    grad_temp_Relu=grad_y_pred.dot(w2.T)\n",
    "    grad_temp=grad_temp_Relu.copy()\n",
    "    grad_w1=X_.T.dot(grad_temp)\n",
    "\n",
    "\n",
    "    # 更新权重, 对w1, w2, b1, b2进行更新\n",
    "    \"\"\" 这里写你的代码 \"\"\"\n",
    "    w1-=learning_rate*grad_w1\n",
    "    w2-=learning_rate*grad_w2\n",
    "    \n",
    "    \n",
    "# 得到最终的w1, w2\n",
    "print('w1={} \\n w2={}'.format(w1, w2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
